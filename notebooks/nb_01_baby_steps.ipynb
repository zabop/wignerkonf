{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/riblidezso/wigner_dl_demo/blob/master/nb_01_baby_steps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlY6-tlM0Pcf"
   },
   "source": [
    "#  Baby steps in deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tORtAyzl-2hK"
   },
   "source": [
    "### Load useful python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSTgGCOC0XX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# plotting and numerical basics\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaRg3s1N0evC"
   },
   "outputs": [],
   "source": [
    "# machine learning baselines\n",
    "# https://scikit-learn.org/stable/\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBGBOvdV1UWO"
   },
   "outputs": [],
   "source": [
    "# keras deep learning framework (with tensorflow backend)\n",
    "# https://keras.io\n",
    "# https://www.tensorflow.org\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import keras.layers as kl\n",
    "import keras.regularizers as kr\n",
    "import keras.optimizers as ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3xs6GXc5UX9"
   },
   "source": [
    "# MNIST handwritten digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOprHOQQ084n"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxYY1xqa4pXi"
   },
   "source": [
    "### Check some handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmFz_b464bvI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 44\n",
    "imshow(x_train[i])\n",
    "print('Label:', y_train[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SV94_cdD5IG6"
   },
   "source": [
    "### Some more info about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWsLOlrq5Mg3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvkn-WsC_fTZ"
   },
   "source": [
    "### Normalize pixel values to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iUtzCbS76B9Z"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_YFFQAb_i4-"
   },
   "source": [
    "## Define functions for 'simple' machine learning baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cxunmr078_MG"
   },
   "outputs": [],
   "source": [
    "def train_baseline(model, x_train, y_train, N_train=60000, **kwargs):\n",
    "  \"\"\"Train a baseline sklearn model.\"\"\"\n",
    "  x_train_flat = x_train.reshape(x_train.shape[0],-1)  # flatten\n",
    "  clf = model(**kwargs)  # init machine learning model\n",
    "  clf.fit(x_train_flat[:N_train],y_train[:N_train])  # train it\n",
    "  return clf\n",
    "\n",
    "\n",
    "def test_baseline(clf, x_test, y_test):\n",
    "  \"\"\"Evaluate a baseline sklearn model.\"\"\"\n",
    "  x_test_flat = x_test.reshape(x_test.shape[0],-1)  # flatten\n",
    "  y_pred = clf.predict(x_test_flat)  # make predictions\n",
    "  acc = np.equal(y_pred, y_test).mean()  # calculate accuracy\n",
    "  print(clf.__class__.__name__, 'accuracy',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKJ-f-BA_pUT"
   },
   "source": [
    "### Test logistic regression\n",
    "\n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "It takes 2-3 minutes, wait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22FhFqH_5dVX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pal/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/pal/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy 0.9201\n",
      "CPU times: user 1min 26s, sys: 329 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = train_baseline(LogisticRegression, x_train, y_train)\n",
    "test_baseline(clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsVCO4fa_-S-"
   },
   "source": [
    "### Test Random Forest\n",
    "\n",
    "[Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "It takes 2-3 minutes, wait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR4-9Pc48VWu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier accuracy 0.9716\n",
      "CPU times: user 4min 55s, sys: 711 ms, total: 4min 56s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = train_baseline(RandomForestClassifier, x_train, y_train,\n",
    "                    n_jobs=-1, n_estimators = 300)\n",
    "test_baseline(clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFnv2QJ_Cs7J"
   },
   "source": [
    "### Test a simple neural network with scikit-learn\n",
    "\n",
    "It takes 1-2 minutes, wait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vj5aM27zP-S2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier accuracy 0.9787\n",
      "CPU times: user 7min 4s, sys: 4min 43s, total: 11min 48s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = train_baseline(MLPClassifier, x_train, y_train,activation='logistic') #multilayerperceptron\n",
    "# logistic is the name of sigmoid func - https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "test_baseline(clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdQhqG88QzYN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the params\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZlPR705Qk1w"
   },
   "source": [
    "### Implement our own neural network with Keras for more control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eq1NvvxPZId"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N1 = 100\n",
    "inp = kl.Input(shape=(28*28,),name='input')  # the input data tensor\n",
    "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
    "x = kl.Dense(10, activation='softmax')(x)  # prediction layer\n",
    "clf = Model(inputs=inp, outputs=x)  # define the model\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "clf.summary()\n",
    "\n",
    "#van száz darab biased parameter, azért nem 78400 hanem 78500 a paraméterek a második layerben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6_7DP1R7EA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.4701 - acc: 0.8734 - val_loss: 0.2605 - val_acc: 0.9262\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.2252 - acc: 0.9364 - val_loss: 0.1893 - val_acc: 0.9443\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1734 - acc: 0.9507 - val_loss: 0.1539 - val_acc: 0.9551\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1419 - acc: 0.9597 - val_loss: 0.1355 - val_acc: 0.9604\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1194 - acc: 0.9664 - val_loss: 0.1209 - val_acc: 0.9649\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1035 - acc: 0.9708 - val_loss: 0.1118 - val_acc: 0.9660\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0903 - acc: 0.9745 - val_loss: 0.1057 - val_acc: 0.9682\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0790 - acc: 0.9780 - val_loss: 0.0977 - val_acc: 0.9717\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0702 - acc: 0.9802 - val_loss: 0.0936 - val_acc: 0.9712\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0618 - acc: 0.9830 - val_loss: 0.0887 - val_acc: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5e32050b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=10,batch_size=256,\n",
    "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))\n",
    "\n",
    "#256 mintából számoljon gradienst és utána lépjen egyet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWVMy5zsUyJN"
   },
   "source": [
    "## Try a larger network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQuMdVS8SnOm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,333,770\n",
      "Trainable params: 1,333,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N1,N2 = 1024,512\n",
    "inp = kl.Input(shape=(28*28,),name='input')\n",
    "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
    "x = kl.Dropout(0.25)(x) # kinullázza előző réteg random elemeit, negyedét használja a második réteg az előző réteg 1024 neuronjának\n",
    "x = kl.Dense(N2, activation='relu')(x)  # first dense layer\n",
    "x = kl.Dropout(0.25)(x) # így a dropout segít az overfit ellen\n",
    "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
    "clf = Model(inputs=inp, outputs=x)\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "clf.summary()\n",
    "\n",
    "# ha túl sok neuron van akkor csak letároljuk a bemenő adatokat és adabázisszerű lesz és nem fog menni új adatokra\n",
    "\n",
    "#tesztnél nem dropoutolunk, csak tanulásnál"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCdVUJwrTPVk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.2657 - acc: 0.9212 - val_loss: 0.1038 - val_acc: 0.9665\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.1024 - acc: 0.9684 - val_loss: 0.0847 - val_acc: 0.9731\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0689 - acc: 0.9781 - val_loss: 0.0690 - val_acc: 0.9787\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.0543 - acc: 0.9825 - val_loss: 0.0719 - val_acc: 0.9774\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.0449 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9823\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0357 - acc: 0.9884 - val_loss: 0.0767 - val_acc: 0.9764\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.0319 - acc: 0.9893 - val_loss: 0.0649 - val_acc: 0.9813\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0256 - acc: 0.9915 - val_loss: 0.0679 - val_acc: 0.9802\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0239 - acc: 0.9924 - val_loss: 0.0732 - val_acc: 0.9800\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0229 - acc: 0.9920 - val_loss: 0.0670 - val_acc: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe60d250ba8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "clf.fit(x_train.reshape(x_train.shape[0],-1),y_train, epochs=10, batch_size=256,\n",
    "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YwDpXiNVa6Z"
   },
   "source": [
    "# Convolutional neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUFwkajOOsFx"
   },
   "outputs": [],
   "source": [
    "# need to create 1 ' color channel'\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "#nem lapítjuk le, megtartjuk 28x28asnak, csak 1 csatorna mert 1 színű a kép"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIDD66cFVjXJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 197,482\n",
      "Trainable params: 197,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N1,N2,N3 = 32,64,128\n",
    "inp = kl.Input(shape=(28,28,1),name='input')\n",
    "x = kl.Conv2D(N1, kernel_size=(3, 3), activation='relu')(inp)\n",
    "x = kl.Conv2D(N1, (3, 3), activation='relu')(x)\n",
    "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
    "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
    "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = kl.Dropout(0.25)(x)\n",
    "x = kl.Flatten()(x)\n",
    "x = kl.Dense(N3, activation='relu')(x)  # first dense layer\n",
    "x = kl.Dropout(0.5)(x)\n",
    "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
    "clf = Model(inputs=inp, outputs=x)\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_ShxAqQWYc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.0772 - acc: 0.9777 - val_loss: 0.0291 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5da7319e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it\n",
    "\n",
    "#clf.fit(x_train,y_train, epochs=10, validation_split=0.1, batch_size=256,\n",
    "#        validation_data = (x_test, y_test))\n",
    "\n",
    "clf.fit(x_train,y_train, epochs=1, batch_size=256,\n",
    "        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxQ_PsbXYgpN"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# 97.5 or 99.4 meh? Let's see a more impressive example: CIFAR10\n",
    "\n",
    "2.5% vs 0.6% not so meh btw\n",
    "\n",
    "[CIFAR10 website](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy5oaMhmY8W8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 78s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "mnist_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
    "                 'horse','ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sKRv3Eyc_C5"
   },
   "source": [
    "## Test yourself at home! \n",
    "\n",
    "I can do around 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMr3HKuQZDPq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 7\n",
    "imshow(x_train[i])\n",
    "plt.axis('off')\n",
    "print('Label:', mnist_classes[ int(y_train[i]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvoY-YSUdVE7"
   },
   "source": [
    "#### Some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlKtkvg6dYaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "strh_HjpdPyG"
   },
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iZrVCLPcPLZ"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWJ17ayRdf3v"
   },
   "source": [
    "### Baselines\n",
    "\n",
    "No Logistic regression because it takes forever!\n",
    "\n",
    "Wait, this also runs form 3-4 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RK6D9lldhjd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier accuracy 0.4668\n",
      "CPU times: user 6min 48s, sys: 636 ms, total: 6min 49s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = train_baseline(RandomForestClassifier, x_train, y_train.flatten(),\n",
    "                    n_jobs=-1, n_estimators = 100)\n",
    "test_baseline(clf, x_test, y_test.flatten())\n",
    "\n",
    "# ez a 46 szazalek nem valami sok, csináljunk inkább konvol hálót, nem azért nem megy mert nem futtattuk elég sokáig azzal amit most csinálunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFDinMXnePj4"
   },
   "source": [
    "# OK, so this is much harder. Let's see neural nets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiemAVx2fRrg"
   },
   "source": [
    "### First a simple MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0yviJBxeO7q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               614600    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 616,610\n",
      "Trainable params: 616,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 2.0159 - acc: 0.2910 - val_loss: 1.8311 - val_acc: 0.3428\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 1.7980 - acc: 0.3670 - val_loss: 1.7570 - val_acc: 0.3794\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 1.7323 - acc: 0.3897 - val_loss: 1.6941 - val_acc: 0.4028\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 1.6778 - acc: 0.4112 - val_loss: 1.6753 - val_acc: 0.4031\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 1.6461 - acc: 0.4208 - val_loss: 1.6231 - val_acc: 0.4210\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 1.6145 - acc: 0.4311 - val_loss: 1.6088 - val_acc: 0.4311\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 1.5934 - acc: 0.4369 - val_loss: 1.5919 - val_acc: 0.4371\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 1.5710 - acc: 0.4442 - val_loss: 1.5839 - val_acc: 0.4361\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 1.5565 - acc: 0.4495 - val_loss: 1.5555 - val_acc: 0.4503\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 1.5385 - acc: 0.4562 - val_loss: 1.5539 - val_acc: 0.4522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5da6f2240>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N1 = 200\n",
    "inp = kl.Input(shape=(32*32*3,),name='input')  # the input data tensor\n",
    "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
    "x = kl.Dense(10, activation='softmax')(x)  # prediction layer\n",
    "clf = Model(inputs=inp, outputs=x)  # define the model\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "# Train it\n",
    "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=10,batch_size=256,\n",
    "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TiSBhhQyigQP"
   },
   "source": [
    "### Maybe a larger one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDzodQKC1Kin"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,676,682\n",
      "Trainable params: 3,676,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 35s 695us/step - loss: 2.0170 - acc: 0.2782 - val_loss: 1.7879 - val_acc: 0.3614\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 30s 604us/step - loss: 1.7991 - acc: 0.3502 - val_loss: 1.6968 - val_acc: 0.3889\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 31s 614us/step - loss: 1.7436 - acc: 0.3724 - val_loss: 1.6441 - val_acc: 0.4167\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 32s 635us/step - loss: 1.6985 - acc: 0.3896 - val_loss: 1.6114 - val_acc: 0.4315\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 32s 649us/step - loss: 1.6729 - acc: 0.3996 - val_loss: 1.5739 - val_acc: 0.4390\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 31s 621us/step - loss: 1.6478 - acc: 0.4078 - val_loss: 1.5572 - val_acc: 0.4480\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 32s 638us/step - loss: 1.6273 - acc: 0.4131 - val_loss: 1.5404 - val_acc: 0.4507\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 29s 578us/step - loss: 1.6060 - acc: 0.4266 - val_loss: 1.5421 - val_acc: 0.4545\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 30s 596us/step - loss: 1.5917 - acc: 0.4293 - val_loss: 1.5244 - val_acc: 0.4647\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 29s 588us/step - loss: 1.5837 - acc: 0.4303 - val_loss: 1.5300 - val_acc: 0.4620\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 31s 628us/step - loss: 1.5672 - acc: 0.4354 - val_loss: 1.4994 - val_acc: 0.4762\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 38s 769us/step - loss: 1.5579 - acc: 0.4393 - val_loss: 1.4967 - val_acc: 0.4664\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1489s 30ms/step - loss: 1.5446 - acc: 0.4459 - val_loss: 1.4815 - val_acc: 0.4793\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 30s 604us/step - loss: 1.5242 - acc: 0.4530 - val_loss: 1.4850 - val_acc: 0.4768\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 33s 656us/step - loss: 1.5225 - acc: 0.4538 - val_loss: 1.4768 - val_acc: 0.4781\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 32s 646us/step - loss: 1.5167 - acc: 0.4546 - val_loss: 1.4609 - val_acc: 0.4874\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 31s 611us/step - loss: 1.5043 - acc: 0.4562 - val_loss: 1.4658 - val_acc: 0.4857\n",
      "Epoch 18/20\n",
      "  512/50000 [..............................] - ETA: 30s - loss: 1.5015 - acc: 0.4531"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dde280d1c387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=20,batch_size=256,\n\u001b[0;32m---> 15\u001b[0;31m         validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N1,N2 = 1024,512\n",
    "inp = kl.Input(shape=(32*32*3,),name='input')\n",
    "x = kl.Dense(N1, activation='relu')(inp)  # first dense layer\n",
    "x = kl.Dropout(0.25)(x)\n",
    "x = kl.Dense(N2, activation='relu')(x)  # first dense layer\n",
    "x = kl.Dropout(0.25)(x)\n",
    "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
    "clf = Model(inputs=inp, outputs=x)\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "# Train it\n",
    "clf.fit(x_train.reshape(x_train.shape[0],-1), y_train, epochs=20,batch_size=256,\n",
    "        validation_data=(x_test.reshape(x_test.shape[0],-1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PN0ugLCIfq5V"
   },
   "source": [
    "## Convolutional, the same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7OCGeddftNq"
   },
   "outputs": [],
   "source": [
    "N1,N2,N3 = 32,64,128\n",
    "inp = kl.Input(shape=(32,32,3),name='input')\n",
    "x = kl.Conv2D(N1, kernel_size=(3, 3), activation='relu')(inp)\n",
    "x = kl.Conv2D(N1, (3, 3), activation='relu')(x)\n",
    "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
    "x = kl.Conv2D(N2, (3, 3), activation='relu')(x)\n",
    "x = kl.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = kl.Dropout(0.25)(x)\n",
    "x = kl.Flatten()(x) \n",
    "x = kl.Dense(N3, activation='relu')(x)  # first dense layer\n",
    "x = kl.Dropout(0.5)(x)\n",
    "x = kl.Dense(10, activation='softmax')(x)  #prediction layers\n",
    "clf = Model(inputs=inp, outputs=x)\n",
    "clf.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6oN6IUEfzAm"
   },
   "outputs": [],
   "source": [
    "# Train it\n",
    "clf.fit(x_train,y_train, epochs=30, validation_split=0.1, batch_size=256,\n",
    "        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMbBdxA_gscg"
   },
   "source": [
    "## State of the art is around 97.88% [ArXiv link](https://arxiv.org/abs/1709.01507)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9bF3o6VnPB6"
   },
   "source": [
    "---\n",
    "\n",
    "# Finally let's implement one of the state of the art networks: [ResNext](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "![resnext](https://cdn-images-1.medium.com/max/1600/1*mdiQTfovOXKnqzfj727b9Q.png)\n",
    "\n",
    "\n",
    "Does not achieve the described results, tell me if you could fix it :) - hol a hiba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiBwri9U0Jc6"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Train resnext of cifar10.\n",
    "The implementation follows the FAIR github repo:\n",
    "https://github.com/facebookresearch/ResNeXt\n",
    "which is slightly different than the arxiv report.\n",
    "Here I use the non-preactivation blocks.\n",
    "The specfic settings (lr,batch size ) can be changed \n",
    "to follow the original values (with enough GPUs, and time).\n",
    "Author: Dezso Ribli\n",
    "\"\"\"\n",
    "\n",
    "CARDINALITY = 2  # 16\n",
    "LR = 0.05  # 0.025\n",
    "BATCH_SIZE = 64  # 32\n",
    "EPOCHS_DROP = [150,225]\n",
    "N_EPOCHS = 250\n",
    "AUG = True\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Activation\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import optimizers\n",
    "import math\n",
    "\n",
    "\n",
    "def resnext(inp, resxt_block, cardinality=4):\n",
    "    \"\"\"Return resnext.\"\"\"\n",
    "    # inital conv\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(5e-4))(inp)\n",
    "    x = Activation('relu')(BatchNormalization()(x))\n",
    "    # residual blocks\n",
    "    x = resxt_blocks(x, resxt_block, cardinality, 64, 256, 1)\n",
    "    x = resxt_blocks(x, resxt_block, cardinality, 128, 512, 2)\n",
    "    x = resxt_blocks(x, resxt_block, cardinality, 256, 1024, 2)\n",
    "    # classifier\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(10,activation='softmax')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resxt_blocks(x, resxt_block, cardinality, n_ch1, n_ch2, init_stride, \n",
    "                 n_block=3):\n",
    "    \"\"\"Perform same size residual blocks.\"\"\"\n",
    "    x_shortcut = Conv2D(n_ch2, (1, 1), strides = init_stride, \n",
    "                        padding='same', kernel_regularizer=l2(5e-4))(x)\n",
    "    x_shortcut = BatchNormalization()(x_shortcut)\n",
    "    # first block\n",
    "    x = resxt_block(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride)\n",
    "    for i in range(n_block-1):  # the other residual blocks\n",
    "        x = resxt_block(x, x, cardinality, n_ch1, n_ch2)\n",
    "    return x  \n",
    "\n",
    "\n",
    "def resxt_block_b(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride=1):\n",
    "    \"\"\"Perform a residual block.\"\"\"\n",
    "    groups=[]\n",
    "    for i in range(cardinality):\n",
    "        y = Conv2D(n_ch1, (1, 1), strides=init_stride, \n",
    "                   kernel_regularizer=l2(5e-4), padding='same')(x)\n",
    "        y = Activation('relu')(BatchNormalization()(y))\n",
    "        y = Conv2D(n_ch1, (3, 3), padding='same', \n",
    "                   kernel_regularizer=l2(5e-4),)(y)\n",
    "        y = Activation('relu')(BatchNormalization()(y))\n",
    "        groups.append(y)\n",
    "    x = keras.layers.concatenate(groups)\n",
    "    x = Conv2D(n_ch2, (1, 1), padding='same', \n",
    "               kernel_regularizer=l2(5e-4),)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = keras.layers.add([x, x_shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x   \n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"Normalize images.\"\"\"\n",
    "    x = x.astype('float32')\n",
    "    x[...,0] = (x[...,0] - x[...,0].mean())/x[...,0].std()\n",
    "    x[...,1] = (x[...,1] - x[...,1].mean())/x[...,1].std()\n",
    "    x[...,2] = (x[...,2] - x[...,2].mean())/x[...,2].std()\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_decay(epoch, base_lr=LR, drop=0.1, epochs_drops=EPOCHS_DROP):\n",
    "    \"\"\"Helper for step learning rate decay.\"\"\"\n",
    "    lrate = base_lr\n",
    "    for epoch_drop in epochs_drops:\n",
    "        lrate *= math.pow(drop,math.floor(epoch/epoch_drop))\n",
    "        return lrate\n",
    "\n",
    "\n",
    "\n",
    "# SGD\n",
    "sgd = optimizers.SGD(lr=LR, decay=0, momentum=0.9, nesterov=True)\n",
    "\n",
    "# resnext\n",
    "res = resnext(Input(shape=(32,32,3)), resxt_block_b, CARDINALITY)\n",
    "res.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=sgd, metrics=['accuracy'])\n",
    "print(res.summary())  # print summary\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = map(norm, (x_train, x_test))  # normalize\n",
    "\n",
    "# train on generator with standard data augmentation\n",
    "gen = ImageDataGenerator(width_shift_range=0.125,\n",
    "                         height_shift_range=0.125,\n",
    "                         horizontal_flip=True)\n",
    "train_generator = gen.flow(x_train, y_train,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "# train\n",
    "res.fit_generator(train_generator, epochs=N_EPOCHS,\n",
    "                  steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  callbacks=[LearningRateScheduler(step_decay)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nb_01_baby_steps.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
