költség függvény, aka Loss, költségfüggvény és modell, minimalizációs program, a&b paramétereket keressük, ax+b-t illesztünk pontokra
deriválunk a szerint b szerint és 0át kapunk akkor vagyunk minimumon #magic

költésfüggvény deriváltja megjelenik egyenletrendszerben

"általában az emberek úgy esnek neki egy problémának hogy nem tudják milyen függvényosztályon keresik a megoldást"

sok paraméter esetén szimbolikus deriválás nem megy computational nehézségek miatt

a megoldandó egyenletrendszer általában nem lineáris #sad

van szimbolikus, numerikus és ___ automatikus ___ deriválás

szimbolikus: ahogy matekon

numerikus: kiértékelem közeli pontokban

automatikus:
pl x=1 értékben szeretném tudni f(x) deriváltját, f(1+epsilon)t kiértékelem, elsőrendű epsilonokat nézem, és megkapom a deriváltat
f(1+epsilon)=(1+epsilon)(1+epsilon)=1+2epsilon
ott a válasz, hogy kettő
epsilon^2=0
valós számokat kiterjesztjük: x -> x+epsilon

ez csak a chain rule

"mi van ha több változóm van... akarunk egy millió változót, nehogymá" xD

tolhatunk így parciális deriválást is #nagyonop

nagyjából Taylor sorozok

sok paraméterből kevés kimenet -> felülről lefelé számoljuk a deriváltat #backpropagation (?)
kevés paraméterből kell sok paraméter -> alulról felfelé költséghatékonyabb

"szokás szerint akkor belefutunk egy NP nehéz problémába"
automatikus deriválás: pontosabb mint a numerikus és jobban scalable mint a szimbolikus

megvannak a deriváltak, nagyon örülünk

utána iterálunk
Newton-Rhapson-nal megyek valamilyen minimumba
skálafaktorral intézzük hogy ne mindig  a  max lépést toljuk -> kell az alpha below:
w_{n+1} = w_n - alpha df/dw (w_n)

probs:
rossz indítás és nincs megoldás
konvergencia can be messed up and we dont go to min
stack lokális minimumba
minimum közelében lecsökkenhet a konvergencia sebessége aka lapos minimum
ha valahol nagy ugrás van a deriváltban az meghülyíti a módszereket
stb


